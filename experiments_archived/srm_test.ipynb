{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import sys \n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from brainiak.isc import isc\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "import brainiak.funcalign.srm\n",
    "from brainiak import image, io\n",
    "\n",
    "import scipy.spatial.distance as sp_distance\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from shared_gpfa import SharedGpfa\n",
    "\n",
    "%autosave 5\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2203 10\n"
     ]
    }
   ],
   "source": [
    "movie_data = np.load(os.path.join('raider', 'movie.npy'))\n",
    "vox_num, nTR, num_subs = movie_data.shape\n",
    "print(vox_num, nTR, num_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "for sub in range(num_subs):\n",
    "    train_data.append(movie_data[:, :nTR//2, sub])    \n",
    "    test_data.append(movie_data[:, -(nTR//2):, sub])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in range(num_subs):    \n",
    "    train_data[sub] = stats.zscore(train_data[sub], axis=1, ddof=1)\n",
    "    train_data[sub] = np.nan_to_num(train_data[sub])\n",
    "    \n",
    "    test_data[sub] = stats.zscore(test_data[sub], axis=1, ddof=1)\n",
    "    test_data[sub] = np.nan_to_num(test_data[sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 50  \n",
    "n_iter = 20 \n",
    "srm = brainiak.funcalign.srm.SRM(n_iter=n_iter, features=features)\n",
    "srm.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SRM: Features X Time-points ', srm.s_.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.tight_layout()\n",
    "plt.imshow(srm.s_, cmap='viridis')\n",
    "plt.title('SRM: Features X Time-points')\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('feature')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(srm.s_[0:3,:].T);\n",
    "plt.title('SRM: top feature')\n",
    "plt.xlabel('TR')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat = sp_distance.squareform(sp_distance.pdist(srm.s_.T))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Distance between pairs of time points in shared space')\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('TR')\n",
    "plt.imshow(dist_mat, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(211)\n",
    "plt.figure()\n",
    "plt.plot(srm.w_[0][0,:])\n",
    "plt.plot(srm.w_[1][0,:])\n",
    "feature_corr = np.corrcoef(srm.w_[0][0,:], srm.w_[1][0,:].T)[0,1]\n",
    "plt.title('SRM: Weights x Features for one voxel (correlation of loading, r: %0.3f)' % feature_corr) \n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('weight for one voxel')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the SRM data into shared space\n",
    "shared_train = srm.transform(train_data)\n",
    "\n",
    "# Zscore the transformed training data\n",
    "for subject in range(num_subs):\n",
    "    shared_train[subject] = stats.zscore(shared_train[subject], axis=1, ddof=1)\n",
    "\n",
    "# Reorganize the data back into an appropriate space for ISC\n",
    "raw_obj = np.zeros((train_data[0].shape[0], train_data[0].shape[1], len(train_data)))\n",
    "for ppt in range(len(train_data)):\n",
    "    raw_obj[:, :, ppt] = train_data[ppt]\n",
    "    \n",
    "# Perform ISC on all participants, collapsing across participants    \n",
    "corr_raw = isc(raw_obj, summary_statistic='mean')\n",
    "corr_raw = np.nan_to_num(corr_raw)  \n",
    "\n",
    "# Reorganize the SRM transformed data back into an appropriate space for ISC\n",
    "shared_obj = np.zeros((shared_train[0].shape[0], shared_train[0].shape[1], len(train_data)))\n",
    "for ppt in range(len(train_data)):\n",
    "    shared_obj[:, :, ppt] = shared_train[ppt]\n",
    "    \n",
    "# Perform ISC on all participants, collapsing across participants        \n",
    "corr_shared = isc(shared_obj, summary_statistic='mean')\n",
    "corr_shared = np.nan_to_num(corr_shared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('ISC for all voxels')\n",
    "plt.hist(corr_raw);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of voxels')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('ISC for shared features')\n",
    "plt.hist(corr_shared);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of features')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "tstat = stats.ttest_ind(np.arctanh(corr_shared), np.arctanh(corr_raw))\n",
    "print('Independent samples t test between raw and SRM transformed data:', tstat.statistic, 'p:', tstat.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data into the shared space using the individual weight matrices\n",
    "shared_test = srm.transform(test_data)\n",
    "\n",
    "# Zscore the transformed test data\n",
    "for subject in range(num_subs):\n",
    "    shared_test[subject] = stats.zscore(shared_test[subject], axis=1, ddof=1)\n",
    "\n",
    "# Reorganize the data back into an appropriate space for ISC\n",
    "raw_obj = np.zeros((test_data[0].shape[0], test_data[0].shape[1], len(test_data)))\n",
    "for ppt in range(len(test_data)):\n",
    "    raw_obj[:, :, ppt] = test_data[ppt]\n",
    "    \n",
    "# Perform ISC on all participants, collapsing across participants    \n",
    "corr_raw = isc(raw_obj, summary_statistic='mean')\n",
    "corr_raw = np.nan_to_num(corr_raw)  \n",
    "\n",
    "# Reorganize the SRM transformed data back into an appropriate space for ISC\n",
    "shared_obj = np.zeros((shared_test[0].shape[0], shared_test[0].shape[1], len(test_data)))\n",
    "for ppt in range(len(test_data)):\n",
    "    shared_obj[:, :, ppt] = shared_test[ppt]\n",
    "    \n",
    "# Perform ISC on all participants, collapsing across participants        \n",
    "corr_shared = isc(shared_obj, summary_statistic='mean')\n",
    "corr_shared = np.nan_to_num(corr_shared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('ISC for all voxels')\n",
    "plt.hist(corr_raw);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of voxels')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('ISC for shared features')\n",
    "plt.hist(corr_shared);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of features')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "tstat = stats.ttest_ind(np.arctanh(corr_shared), np.arctanh(corr_raw))\n",
    "print('Independent samples t test between raw and SRM transformed data:', tstat.statistic, 'p:', tstat.pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = srm.w_[0]  # Weights for subject 1\n",
    "signal_srm0 = w0.dot(shared_test[0])  # Reconstructed signal for subject 1\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title('SRM reconstructed vs. original signal for one voxel', fontsize=14)\n",
    "plt.plot(signal_srm0[100,:100])\n",
    "plt.plot(test_data[0][100,:100])\n",
    "plt.xlabel('TR')\n",
    "plt.ylabel('signal of one voxel')\n",
    "plt.legend(('Reconstructed data', 'Original data'), loc=(1.04,0.5))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the reconstruction on all individual participants and organize it for ISC\n",
    "signal_srm = np.zeros((test_data[0].shape[0], test_data[0].shape[1], len(test_data)))\n",
    "for ppt in range(len(test_data)):\n",
    "    signal_srm[:, :, ppt] = w0.dot(shared_test[ppt])\n",
    "\n",
    "corr_reconstructed = isc(signal_srm, summary_statistic='mean')\n",
    "corr_reconstructed = np.nan_to_num(corr_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the figure\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('ISC for all voxels')\n",
    "plt.hist(corr_raw);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of voxels')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('ISC for shared features')\n",
    "plt.hist(corr_reconstructed[0]);\n",
    "plt.xlabel('correlation')\n",
    "plt.ylabel('number of features')\n",
    "plt.xlim([-1, 1]);\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "tstat = stats.ttest_1samp(np.arctanh(corr_reconstructed) - np.arctanh(corr_raw), 0)\n",
    "print('Dependent samples t test between raw and SRM transformed data:', tstat.statistic, 'p:', tstat.pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Time-segment matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take in a list of participants of voxel by TR data. Also specify how big the time segment is to be matched\n",
    "def time_segment_matching(data, win_size=10): \n",
    "    nsubjs = len(data)\n",
    "    (ndim, nsample) = data[0].shape\n",
    "    accu = np.zeros(shape=nsubjs)\n",
    "    nseg = nsample - win_size \n",
    "    \n",
    "    # mysseg prediction\n",
    "    trn_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "    \n",
    "    # the training data also include the test data, but will be subtracted when calculating A\n",
    "    for m in range(nsubjs):\n",
    "        for w in range(win_size):\n",
    "            trn_data[w*ndim:(w+1)*ndim,:] += data[m][:,w:(w+nseg)]\n",
    "    for tst_subj in range(nsubjs):\n",
    "        tst_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "        for w in range(win_size):\n",
    "            tst_data[w*ndim:(w+1)*ndim,:] = data[tst_subj][:,w:(w+nseg)]\n",
    "\n",
    "        A =  np.nan_to_num(stats.zscore((trn_data - tst_data),axis=0, ddof=1))\n",
    "        B =  np.nan_to_num(stats.zscore(tst_data,axis=0, ddof=1))\n",
    "\n",
    "        # compute correlation matrix\n",
    "        corr_mtx = compute_correlation(B.T,A.T)\n",
    "\n",
    "        # The correlation classifier.\n",
    "        for i in range(nseg):\n",
    "            for j in range(nseg):\n",
    "                # exclude segments overlapping with the testing segment\n",
    "                if abs(i-j)<win_size and i != j :\n",
    "                    corr_mtx[i,j] = -np.inf\n",
    "        max_idx =  np.argmax(corr_mtx, axis=1)\n",
    "        accu[tst_subj] = sum(max_idx == range(nseg)) / nseg\n",
    "\n",
    "        # Print accuracy\n",
    "#         print(\"Accuracy for subj %d is: %0.4f\" % (tst_subj, accu[tst_subj] ))\n",
    "        \n",
    "    print(\"The average accuracy among all subjects is {0:f} +/- {1:f}\".format(np.mean(accu), np.std(accu)))\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitly double dipping!\n",
    "\n",
    "accu_train_r = time_segment_matching(train_data, win_size=10)\n",
    "accu_train_s = time_segment_matching(shared_train, win_size=10)\n",
    "accu_test_r = time_segment_matching(test_data, win_size=10)\n",
    "accu_test_s = time_segment_matching(shared_test, win_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrected Time-segment Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Take in a list of participants of voxel by TR data. Also specify how big the time segment is to be matched\n",
    "def correlation_classifier(train, test, win_size): \n",
    "    nsubjs = len(train)\n",
    "    (ndim, nsample) = train[0].shape\n",
    "    nseg = nsample - win_size \n",
    "    \n",
    "    # mysseg prediction\n",
    "    trn_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "    \n",
    "    for m in range(nsubjs):\n",
    "        for w in range(win_size):\n",
    "            trn_data[w*ndim:(w+1)*ndim,:] += train[m][:,w:(w+nseg)]        \n",
    "        \n",
    "    tst_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "    for w in range(win_size):\n",
    "        tst_data[w*ndim:(w+1)*ndim,:] = test[:,w:(w+nseg)]\n",
    "\n",
    "    # compute correlation matrix\n",
    "    corr_mtx = compute_correlation(trn_data.T,tst_data.T)\n",
    "\n",
    "    # The correlation classifier.\n",
    "    for i in range(nseg):\n",
    "        for j in range(nseg):\n",
    "            # exclude segments overlapping with the testing segment\n",
    "            if abs(i-j)<win_size and i != j :\n",
    "                corr_mtx[i,j] = -np.inf\n",
    "    max_idx =  np.argmax(corr_mtx, axis=1)\n",
    "    accu = sum(max_idx == range(nseg)) / nseg\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_segment_matching(half1, half2, n_features=50, win_size=10, desc=''):\n",
    "    if type(win_size) is int: win_size = [win_size]\n",
    "    loo = LeaveOneOut()\n",
    "    acc_r = []\n",
    "    acc_s = []\n",
    "    acc_g = []\n",
    "\n",
    "    pbar = tqdm(loo.split(half1), total=loo.get_n_splits(half1), desc=desc)\n",
    "    for train_subs, test_sub in pbar:\n",
    "        test_sub = test_sub[0]\n",
    "\n",
    "        # no functional registration\n",
    "        pbar.set_description(f'{desc} Raw')\n",
    "        pbar.refresh()\n",
    "        # time-segment matching performance\n",
    "        sub_acc = [correlation_classifier(half2[train_subs], half2[test_sub], w) for w in win_size]\n",
    "        acc_r.append(sub_acc)\n",
    "\n",
    "        \n",
    "        # Shared Response Model\n",
    "        pbar.set_description(f'{desc} SRM')\n",
    "        pbar.refresh()\n",
    "        # fit srm using half1[train_subs]\n",
    "        srm = brainiak.funcalign.srm.SRM(n_iter=20, features=n_features, rand_seed=0)\n",
    "        srm.fit(half1[train_subs])\n",
    "        # held-out subject base using half1[test_sub]\n",
    "        w_test_sub = srm.transform_subject(half1[test_sub])\n",
    "        # map half2[train_subs] to shared space\n",
    "        half2_shared_train = srm.transform(half2[train_subs])\n",
    "        # map held-out subject half2 to shared space\n",
    "        half2_shared_test = w_test_sub.T.dot(half2[test_sub])\n",
    "        # time-segment matching performance\n",
    "        sub_acc = [correlation_classifier(half2_shared_train, half2_shared_test, w) for w in win_size]\n",
    "        acc_s.append(sub_acc)\n",
    "        \n",
    "        # Shared GPFA\n",
    "        pbar.set_description(f'{desc} SGPFA')\n",
    "        pbar.refresh()\n",
    "        # fit sgpfa using half1[train_subs]\n",
    "        sgpfa = SharedGpfa(len(train_subs), half1.shape[1], n_features)\n",
    "        sgpfa.fit(train_data=half1[train_subs], n_iters=1100, learning_rate=0.075, fa_init=True, reg=0.)\n",
    "        # map half2[train_subs] to shared space\n",
    "        half2_shared_train = sgpfa.add_video(half2[train_subs], n_iters=400, learning_rate=0.06, ls_init=False, desc='mapping 2nd half for train')\n",
    "        # held-out subject base using half1[test_sub]\n",
    "        w_test_sub = sgpfa.add_subject(half1[test_sub], add_to_model=True, solve_ls=True)\n",
    "        # map held-out subject half2 to shared space\n",
    "        ## TODO: what about the noise variance for the treaining subject?? \n",
    "        half2_shared_test = sgpfa.add_video(half2[test_sub], n_iters=300, learning_rate=0.06, ls_init=False, subs=sgpfa.m, desc='mapping 2nd half for test')\n",
    "        half2_shared_test = half2_shared_test[0]\n",
    "\n",
    "        # time-segment matching performance\n",
    "        sub_acc = [correlation_classifier(half2_shared_train, half2_shared_test, w) for w in win_size]\n",
    "        acc_g.append(sub_acc)\n",
    "        \n",
    "        for i in range(len(acc_g[-1])):\n",
    "            print(f'w = {win_size[i]} \\t-- SGPFA: {acc_g[-1][i]:.2f} \\t SRM: {acc_s[-1][i]:.2f} \\t Raw: {acc_r[-1][i]:.2f}')\n",
    "        \n",
    "    acc_s = np.array(acc_s)\n",
    "    acc_r = np.array(acc_r)\n",
    "    acc_g = np.array(acc_g)\n",
    "\n",
    "    return acc_g, acc_s, acc_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84dbc2efb2d4868a53125b125f9de18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd6640df9c4c908094379573901466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='fitting SGPFA', max=1100.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-610c6671e63e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0macc_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_segment_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The average accuracy among all subjects using SGPFA is:  {acc_g.mean(0)} +/- {acc_g.std(0)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f52df393c151>\u001b[0m in \u001b[0;36mtime_segment_matching\u001b[0;34m(half1, half2, n_features, win_size, desc)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# fit sgpfa using half1[train_subs]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0msgpfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSharedGpfa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msgpfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_subs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.075\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfa_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m# map half2[train_subs] to shared space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mhalf2_shared_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgpfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhalf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_subs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.06\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mapping 2nd half for train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/tigrlab/projects/rebrahimi/shared-gpfa/shared_gpfa.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, n_iters, learning_rate, tensorboard, fa_init, smoothness, reg, desc, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sgpfa/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sgpfa/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sgpfa/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sgpfa/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sgpfa/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/sgpfa/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.conda/envs/sgpfa/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "half1 = np.stack(train_data, 0)\n",
    "half2 = np.stack(test_data, 0)\n",
    "\n",
    "n = 3\n",
    "acc_g, acc_s, acc_r = time_segment_matching(half1[:n, :, :75], half2[:n, :, :75], n_features=40, win_size=[20])\n",
    "\n",
    "print(f'The average accuracy among all subjects using SGPFA is:  {acc_g.mean(0)} +/- {acc_g.std(0)}')\n",
    "print(f'The average accuracy among all subjects using SRM is:    {acc_s.mean(0)} +/- {acc_s.std(0)}')\n",
    "print(f'The average accuracy among all subjects using RAW is:    {acc_r.mean(0)} +/- {acc_r.std(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsubs = np.arange(3, len(half1) + 1)\n",
    "acc_r = []\n",
    "acc_s = []\n",
    "acc_g = []\n",
    "win_size=[3, 5, 10, 20, 30]\n",
    "\n",
    "for n in nsubs:\n",
    "    _acc_g, _acc_s, _acc_r = time_segment_matching(half1[:n, :, :75], half2[:n, :, :75], n_features=40, desc=f'{n} subjects', win_size=win_size)\n",
    "    print(f'The average accuracy among all subjects using SGPFA is:  {_acc_g.mean(0)} +/- {_acc_g.std(0)}')\n",
    "    print(f'The average accuracy among all subjects using SRM is:    {_acc_s.mean(0)} +/- {_acc_s.std(0)}')\n",
    "    print(f'The average accuracy among all subjects using RAW is:    {_acc_r.mean(0)} +/- {_acc_r.std(0)}')\n",
    "    acc_r.append(_acc_r)\n",
    "    acc_s.append(_acc_s)\n",
    "    acc_g.append(_acc_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "ncols = 2\n",
    "nrows = len(win_size) // ncols\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(10*ncols,5*nrows))\n",
    "\n",
    "for w, ax in zip(range(len(win_size)), axs):\n",
    "    for acc, label in zip((acc_g, acc_s, acc_r), ('SGPFA', 'SRM', 'Raw')):\n",
    "        y = list(map(lambda x: x.mean(0), acc))[w]\n",
    "        yerr = list(map(lambda x: x.std(0), acc))[w]\n",
    "        ax.errorbar(nsubs, y=y , yerr=yerr, label=label, marker='o', capsize=3, elinewidth=1.5)\n",
    "    ax.grid(color='w')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Number of subjects')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f'window size: {win_size[w]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [\n",
    "    [[0.31481481, 0.5047619 , 0.82051282, 0.93333333, 1.        ], [0.07550697, 0.10583862, 0.01918799, 0.0942809 , 0.        ]],\n",
    "    [[0.31018519, 0.47142857, 0.76410256, 0.96969697, 1.        ], [0.06835566, 0.09965928, 0.05664288, 0.04285496, 0.        ]],\n",
    "    [[0.11574074, 0.21428571, 0.43076923, 0.68484848, 0.98518519], [0.06245712, 0.07648752, 0.16426274, 0.03090315, 0.02095131]],\n",
    "    [[0.33680556, 0.56071429, 0.82307692, 0.95454545, 1.        ], [0.07039283, 0.08885611, 0.07956985, 0.0522233 , 0.        ]],\n",
    "    [[0.36111111, 0.53928571, 0.84230769, 0.96363636, 1.        ], [0.09771699, 0.09112393, 0.10568178, 0.06298367, 0.        ]],\n",
    "    [[0.17361111, 0.30357143, 0.53461538, 0.77272727, 1.        ], [0.0625    , 0.06952829, 0.15890714, 0.04165978, 0.        ]],\n",
    "    [[0.35833333, 0.58      , 0.88615385, 0.99272727, 1.        ], [0.08117577, 0.06916411, 0.07511727, 0.01454545, 0.        ]],\n",
    "    [[0.37777778, 0.57714286, 0.86153846, 0.99272727, 1.        ], [0.09018157, 0.1329723 , 0.11051252, 0.01454545, 0.        ]],\n",
    "    [[0.19444444, 0.30285714, 0.53846154, 0.85818182, 1.        ], [0.062113  , 0.07845446, 0.1513646 , 0.08635885, 0.        ]],\n",
    "    [[0.36574074, 0.60714286, 0.87179487, 0.97272727, 1.        ], [0.04721314, 0.02945075, 0.09216513, 0.06098367, 0.        ]],\n",
    "    [[0.40277778, 0.61666667, 0.88461538, 0.98787879, 1.        ], [0.10111264, 0.1152981 , 0.10723345, 0.02710385, 0.        ]],\n",
    "    [[0.17361111, 0.27619048, 0.47435897, 0.82727273, 1.        ], [0.06551376, 0.12027934, 0.19488866, 0.10271789, 0.        ]],\n",
    "    [[0.39484127, 0.6244898 , 0.9010989 , 1.        , 1.        ], [0.05027325, 0.09122244, 0.08540378, 0.        , 0.        ]],\n",
    "    [[0.42460317, 0.62244898, 0.8967033 , 1.        , 1.        ], [0.0938641 , 0.11875377, 0.11266818, 0.        , 0.        ]],\n",
    "    [[0.18452381, 0.27755102, 0.50549451, 0.85454545, 1.        ], [0.06459849, 0.12070182, 0.16841533, 0.12559187, 0.        ]],\n",
    "    [[0.40277778, 0.60357143, 0.88461538, 1.        , 1.        ], [0.05379144, 0.04559695, 0.09036415, 0.        , 0.        ]],\n",
    "    [[0.41666667, 0.63035714, 0.875     , 0.99090909, 1.        ], [0.09547033, 0.10137763, 0.13618589, 0.02405228, 0.        ]],\n",
    "    [[0.20486111, 0.29821429, 0.55192308, 0.79545455, 1.        ], [0.07818285, 0.11052692, 0.12904692, 0.15844023, 0.        ]],\n",
    "    [[0.41358025, 0.6015873 , 0.89059829, 0.9979798 , 1.        ], [0.0514609 , 0.06749299, 0.09660767, 0.00571399, 0.        ]],\n",
    "    [[0.41975309, 0.61587302, 0.88205128, 0.99393939, 1.        ], [0.0817173 , 0.11979659, 0.13548559, 0.01714198, 0.        ]],\n",
    "    [[0.19444444, 0.3       , 0.56239316, 0.77979798, 1.        ], [0.05892557, 0.09965928, 0.16169892, 0.17279519, 0.        ]],\n",
    "    [[0.43194444, 0.63714286, 0.89846154, 0.99272727, 1.        ], [0.0649816 , 0.07261866, 0.08233285, 0.02181818, 0.        ]],\n",
    "    [[0.42361111, 0.62714286, 0.88923077, 0.99454545, 1.        ], [0.09027778, 0.11472238, 0.13461319, 0.01636364, 0.        ]],\n",
    "    [[0.19861111, 0.31571429, 0.56769231, 0.77818182, 1.        ], [0.05009636, 0.10110613, 0.14340739, 0.15487665, 0.        ]]\n",
    "]\n",
    "\n",
    "z = np.array(z)\n",
    "\n",
    "win_size = [3, 5, 10, 20, 30]\n",
    "nsubs = np.arange(3, 11)\n",
    "fig, axs = plt.subplots(3, 2, figsize=(30, 20))\n",
    "jitter = 0.015\n",
    "\n",
    "for i, w, ax in zip(range(len(win_size)), win_size, axs.flat):\n",
    "    for j, label in zip(range(3), ('SGPFA', 'SRM', 'Raw')):\n",
    "        ax.errorbar(nsubs+j*jitter, y=z[j::3, 0, i], yerr=z[j::3, 1, i], label=label, marker='o', capsize=3, elinewidth=1.5, alpha=1)\n",
    "    ax.grid(color='w')\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Number of subjects')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_title(f'window size: {w}')\n",
    "axs.ravel()[-1].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
